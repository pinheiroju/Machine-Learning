{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GyWyCRVddCG"
   },
   "source": [
    "# CS549-02 Machine Learning Spring 2024: Irfan Khan\n",
    "# Assignment 2: Linear Regression\n",
    "\n",
    "\n",
    "**Total: 10 points**\n",
    "\n",
    "In this assignment, you will solve a linear regression problem using both the normal equation method and the gradient descent method.\n",
    "\n",
    "Once you have obtained the solution, you will be able to predict the output for new data.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "Suppose you are responsible for advertizing for a company. You need a method to predict increase in revenue as a function\n",
    "of money spent on advertisement. So, if you want to request a certain advertisement budget, you would like to show\n",
    "a prediction of the revenue increase with the requested advertisement budget.\n",
    "\n",
    "You have collected sample data from past in an excel file A2data that contains two columns. Column A contains the money spent\n",
    "on advertisement in thousands of dollars. Column B contains the increase in revenue in thousands of dollars. Note that there are some outliers in the data where the revenue decreased even with an increase of advertisement dollars.\n",
    "\n",
    "The task is to build a linear regression model that predicts the increase in revenue as a function of advertisement dollars sepnt. \n",
    "- Task 1) Train the model using Normal Equation method.\n",
    "- Task 2) Train the model using Gradient Descent method.\n",
    "- Task 3) Look at the convergence impact for different learning rates for Gradient Descent $\\alpha$s. \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv # Used for computing the inverse of matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 (2 point)\n",
    "\n",
    "Load data from the A2data spreadsheet into a NumPy array. Visualize data in a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 587,
     "status": "error",
     "timestamp": 1643571209537,
     "user": {
      "displayName": "Naga Vamsi Yarlagadda",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16254615739245089120"
     },
     "user_tz": 480
    },
    "id": "YNi1mkL8ddCd",
    "outputId": "949c5a29-ffe6-4fcd-f132-16f509aeb2dd"
   },
   "outputs": [],
   "source": [
    "# Load data \n",
    "\n",
    "# Load data from Excel into a pandas DataFrame\n",
    "df = pd.read_excel('A2data.xlsx')\n",
    "\n",
    "# Extract the two columns and convert them to a NumPy array\n",
    "data = df[['x', 'y']].to_numpy()\n",
    "\n",
    "print('shape of original data:', data.shape) # Check if data is 97 by 2\n",
    "\n",
    "\n",
    "#Start your code\n",
    "\n",
    "#It is a good idea to visualize data on a scatter plot, if possible. Here we can.\n",
    "#Plot original data\n",
    "\n",
    "\n",
    "\n",
    "#End your code\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Output\n",
    "\n",
    "shape of original data: (97, 2)<br>\n",
    "\n",
    "<img src=\"A2image1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-Rd2HFhddCh"
   },
   "source": [
    "---\n",
    "## 2 (4 points)\n",
    "\n",
    "Implement the Normal Equation method for linear regression: $\\theta = (X'^T X')^{-1}X'^T y$\n",
    "\n",
    "Use the learned $\\theta$ to make predictions: $\\hat{y} = X'\\theta$\n",
    "\n",
    "Compute the residual sum of squares of the model: $RSS = \\sum_i (\\hat{y}^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Draw the straight line representing linear regression on the scatter plot\n",
    "\n",
    "Compute predicted revenue increase for an $18000 advertisement budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A3Zg4ZwddCk",
    "outputId": "09247d50-62c8-4478-dea7-18e0580a4672"
   },
   "outputs": [],
   "source": [
    "# Create matrix Xprime and y\n",
    "# Xprime has two columns: \n",
    "#   - The first column contain all 1s, which is for the intercept\n",
    "#   - The second column contain feature, i.e., the 1st column of data_norm\n",
    "#   - y has one column, i.e., the 2nd column of data_norm\n",
    "\n",
    "Xprime = np.ones_like(data)\n",
    "\n",
    "Xprime[:,1] = data[:, 0]\n",
    "y = data[:,-1]\n",
    "\n",
    "# Normalize data - better for convergence\n",
    "\n",
    "\n",
    "#### START YOUR CODE ####\n",
    "\n",
    "# Compute theta_method1 using normal equation method\n",
    "# Hint: use the inv() function imported from numpy.linalg\n",
    "\n",
    "\n",
    "# Use the theta obtained to make predictions and compute the residuals\n",
    "# Hint: use numpy.dot() and numpy.sum(), and avoid using for loops\n",
    "#Compute \"y_hat\"\n",
    "\n",
    "\n",
    "# Compute residuals \"RSS1\"\n",
    "\n",
    "\n",
    "#Predict increase in revenue with $18000 advertisement budget, \"y_pred\"\n",
    "\n",
    "\n",
    "#Draw a straightline representing linear regression on the scatter plot with sample data\n",
    "\n",
    "\n",
    "\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "print('Theta obtained from normal equation:', theta_method1)\n",
    "print('Residual sum of squares (RSS): ', RSS1)\n",
    "print ('Predicted increase in revenue with $18000 advertisement budget: $', round(y_pred*1000,2))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Output\n",
    "\n",
    "<img src=\"A2image2.png\">\n",
    "\n",
    "Theta obtained from normal equation: [-38.95780878  11.93033644]<br>\n",
    "Residual sum of squares (RSS):  86853.24469391846<br>\n",
    "Predicted increase in revenue with $\\$18000$ advertisement budget is $\\$175788.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IPElsmYddCo"
   },
   "source": [
    "## 3 (4 points)\n",
    "\n",
    "Implement the Gradient Descent method for linear regression with one feature.\n",
    "\n",
    "The cost function: $J(b, w) = \\frac{1}{2m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2m}\\sum_i (b + w x_1^{(i)}-y^{(i)})^2$\n",
    "\n",
    "Gradients w.r.t. parameters: $\\frac{\\partial J}{\\partial \\theta} = \\begin{cases}\\frac{\\partial J}{\\partial b}\\\\ \\frac{\\partial J}{\\partial w}\\\\ \\end{cases} = \\begin{cases}\\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})\\\\ \\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})x_1^{(i)}\\\\\\end{cases}$\n",
    "\n",
    "The formula to update parameters at each iteration: $\\theta := \\theta - \\alpha * \\frac{\\partial J}{\\partial \\theta}$\n",
    "\n",
    "Note that $X$, $y$, and $\\theta$ are all vectors (numpy arrays), and thus the operations above should be implemented in a vectorized fashion. Use `numpy.sum()`, `numpy.dot()` and other vectorized functions, and avoid writing `for` loops in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Hd9h8oBddCu"
   },
   "outputs": [],
   "source": [
    "# Define the gradientDescent function\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Params\n",
    "        X - Shape: (m,2); m is the number of data examples\n",
    "        y - Shape: (m,)\n",
    "        theta - Shape: (2,)\n",
    "        num_iters - Maximum number of iterations\n",
    "    Return\n",
    "        A tuple: (theta, RSS, cost_array)\n",
    "        theta - the learned model parameters\n",
    "        RSS - residual sum of squares\n",
    "        cost_array - stores the cost value of each iteration. Its shape is (num_iters,)\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_array =[]\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "        #### START YOUR CODE ####\n",
    "        # Make predictions\n",
    "        # Shape of y_hat: m by 1\n",
    "        \n",
    "        \n",
    "        # Compute the difference between prediction (y_hat) and ground truth label (y)\n",
    "        diff = y_hat - y\n",
    "\n",
    "        # Compute the cost\n",
    "        # Hint: Use the diff computed above\n",
    "        \n",
    "\n",
    "        # Compute gradients\n",
    "        # Hint: Use the diff computed above\n",
    "        # Hint: Shape of gradients is the same as theta\n",
    "       \n",
    "\n",
    "        # Update theta\n",
    "        \n",
    "        #### END YOUR CODE ####\n",
    "    \n",
    "    # Compute residuals\n",
    "    # Hint: Should use the same code as in #2 above\n",
    "    #### START YOUR CODE ####\n",
    "    \n",
    "    \n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "    return theta, RSS, cost_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY1Z4R1ZddCy",
    "outputId": "9627d299-88e8-439d-f201-c75e3b6d4535"
   },
   "outputs": [],
   "source": [
    "# This cell is to evaluate the gradientDescent function implemented above\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Define learning rate and maximum iteration number\n",
    "ALPHA = 0.01\n",
    "MAX_ITER = 5000\n",
    "\n",
    "# Initialize theta to [0,0]\n",
    "theta = np.zeros(2)\n",
    "theta_method2, RSS2, cost_array = gradientDescent(Xprime, y, theta, ALPHA, MAX_ITER)\n",
    "\n",
    "print('Theta obtained from gradient descent:', theta_method2)\n",
    "print('Residual sum of squares (RSS): ', RSS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected Output\n",
    "\n",
    "Theta obtained from gradient descent: [-38.95300511  11.92985386]<br>\n",
    "Residual sum of squares (RSS):  86853.24510146641<br>\n",
    "Check to see that the result is the close to that obtained via normal equation method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDUBRPJrddC3"
   },
   "source": [
    "## 4\n",
    "\n",
    "\n",
    "Plot the cost against iteration number. This is a common method of examining the performance of gradient descent.\n",
    "\n",
    "Try different values of learning rate, e.g., $\\alpha=\\{0.01, 0.005, 0.05\\}$, and see how the cost curves change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uwnMwPHPddC6",
    "outputId": "f5736629-edf7-4b06-fd48-f50652803816"
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "MAX_ITER = 1000\n",
    "theta = np.zeros(2)\n",
    "\n",
    "_, _, cost_array = gradientDescent(Xprime, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N6jJicF0ddC8",
    "outputId": "3621acd2-15b2-4329-c6c8-176be87568f2"
   },
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "MAX_ITER = 1000\n",
    "theta = np.zeros(2)\n",
    "_, _, cost_array = gradientDescent(Xprime, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHop4M3XddC_",
    "outputId": "ee1617d3-a116-4112-842b-43a29a1df0e5"
   },
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "MAX_ITER = 1000\n",
    "theta = np.zeros(2)\n",
    "_, _, cost_array = gradientDescent(Xprime, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iKQaaEpSddDB"
   },
   "source": [
    "Note that with smaller $\\alpha$, convergence is slower. At iteration 1000, for $\\alpha = 0.01$, cost is <500 but for $\\alpha = 0.001$, cost is > 500. Too large an alpha may result in no convergence as shown above for $\\alpha= 0.05$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "a2_linear_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
