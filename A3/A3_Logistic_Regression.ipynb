{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a66f146",
   "metadata": {},
   "source": [
    "# CS549-02 Machine Learning Spring 2024: Irfan Khan\n",
    "# Assignment 3: Logistic Regression\n",
    "\n",
    "\n",
    "**Total: 10 points**\n",
    "\n",
    "In this assignment, you will implement logistic regression and apply it to a dataset to help make admission decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2706c9",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Suppose that you run a college admission preparation company and you want to determine each of your student’s chance of admission in SDSU based on their results on two exams you administer. The two exams are similar to MATH SAT and Verbal SAT exams\n",
    "\n",
    "You have historical data from previous students that you can use as a training set for logistic regression. For each training example, you have the student’s scores and the admissions decision.\n",
    "Your task is to build a classification model that estimates an applicant’s probability of admission based on the two scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e42ee2",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b883e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "#math will come in handy later to implement the ceiling function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9acfd34",
   "metadata": {},
   "source": [
    "## 1 Load Data (1 point)\n",
    "A3data excel file, containing sample data, has three columns, the first column has the Exam1 score, the 2nd column has the Exam2 score and the third column has the admission decision from SDSU (1: admit, 0: not admit). Load data from Excel spreadsheet A3data into X_train and y_train. Show the data on a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start your code\n",
    "#Hint - use pandas to read the Excel file data and then extract the data to a nump array \"data\"\n",
    "\n",
    "\n",
    "\n",
    "#End your code\n",
    "\n",
    "print('shape of sample data:', data.shape) # Check if data is 100 by 3\n",
    "\n",
    "#Load data into X_train a numpy array of shape (100,2) and y_train of shape (100,1)\n",
    "#Start your code\n",
    "\n",
    "\n",
    "\n",
    "#End your code\n",
    "\n",
    "#It is a good idea to visualize data on a scatter plot, if possible. Here we can.\n",
    "x_class0 = X_train[y_train == 0]\n",
    "x_class1 = X_train[y_train == 1]\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x_class0[:, 0], x_class0[:, 1], color='blue', label='Not Admitted')\n",
    "plt.scatter(x_class1[:, 0], x_class1[:, 1], color='red', label='Admitted')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Exam1 Score')\n",
    "plt.ylabel('Exam2 Score')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fedd361",
   "metadata": {},
   "source": [
    "# Expected Result\n",
    "\n",
    "shape of sample data: (100, 3)<br>\n",
    "\n",
    "<img src=\"A3image1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae321236",
   "metadata": {},
   "source": [
    "## 2 (1 point)\n",
    "\n",
    "Implement the Sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb43034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "          \n",
    "    ### START CODE HERE ### \n",
    "    \n",
    "   \n",
    "    \n",
    "    ### END SOLUTION ###  \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e18b55",
   "metadata": {},
   "source": [
    "# 3 (2 points)\n",
    "\n",
    "Implement the cost function for Logistic Regression. We will use the b, w notation representing bias and weights respectively, instead of $\\theta_{0}$ and $\\theta_{i}$, $i=1,...$\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is - \n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function.\n",
    "    * It might be helpful to first calculate an intermediate variable $z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b = w_0x^{(i)}_0 + ... + w_{n-1}x^{(i)}_{n-1} + b$ where $n$ is the number of features, before calculating $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f42b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: compute_cost\n",
    "def compute_cost(X, y, w, b, *argv):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      *argv : unused, for this assignment\n",
    "    Returns:\n",
    "      total_cost : (scalar) cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d962f82a",
   "metadata": {},
   "source": [
    "Check to see if you have implemented sigmoid and compute_cost functions correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135871f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display cost with non-zero w and b\n",
    "test_w = np.array([0.2, 0.3])\n",
    "test_b = -14.\n",
    "cost = compute_cost(X_train, y_train, test_w, test_b)\n",
    "\n",
    "print('Cost at test w =[0.2,0.3] and b = -14: {:.3f}'.format(cost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe376bc",
   "metadata": {},
   "source": [
    "# Expected Result\n",
    "\n",
    "Cost at test w =[0.2,0.3] and b = -14: 5.117"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd668fdf",
   "metadata": {},
   "source": [
    "## 4 (3 points)\n",
    "\n",
    "Implement Gradient for Logistic Regression\n",
    "\n",
    "Recall that the gradient descent algorithm is:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$, $w_j$ are all updated simultaniously.\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction, while $y^{(i)}$ is the actual label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e67ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, *argv): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      *argv : unused, for compatibility with regularized version below\n",
    "    Returns\n",
    "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    \n",
    "    \n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7a7cf",
   "metadata": {},
   "source": [
    "# Implement Gradient Descent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb57999",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No additional code needed. Just Run this cell\n",
    "\n",
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (ndarray Shape (m, n) data, m examples by n features\n",
    "      y :    (ndarray Shape (m,))  target value \n",
    "      w_in : (ndarray Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)              Initial value of parameter of the model\n",
    "      cost_function :              function to compute cost\n",
    "      gradient_function :          function to compute gradient\n",
    "      alpha : (float)              Learning rate\n",
    "      num_iters : (int)            number of iterations to run gradient descent\n",
    "      lambda_ : (scalar, float)    regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (ndarray Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013f2ff7",
   "metadata": {},
   "source": [
    "## Run Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29702a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No additional code needed. Just run this cell\n",
    "np.random.seed(1)\n",
    "initial_w = 0.01 * (np.random.rand(2) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 0.001\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                                   compute_cost, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e902acd3",
   "metadata": {},
   "source": [
    "# Expected Output\n",
    "\n",
    "Iteration    0: Cost     0.96   \n",
    "Iteration 1000: Cost     0.31<br>\n",
    "Iteration 2000: Cost     0.30<br>\n",
    "Iteration 3000: Cost     0.30<br>\n",
    "Iteration 4000: Cost     0.30<br>\n",
    "Iteration 5000: Cost     0.30  \n",
    "Iteration 6000: Cost     0.30<br>\n",
    "Iteration 7000: Cost     0.30<br>\n",
    "Iteration 8000: Cost     0.30<br>\n",
    "Iteration 9000: Cost     0.30<br>\n",
    "Iteration 9999: Cost     0.30  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b72928",
   "metadata": {},
   "source": [
    "## Plot the Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No additional Code is needed. Just run this cell\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(x_class0[:, 0], x_class0[:, 1], color='blue', label='Not Admitted')\n",
    "plt.scatter(x_class1[:, 0], x_class1[:, 1], color='red', label='Admitted')\n",
    "#Plot decision boundry\n",
    "x1 = [30,40,50,60,70,80,90,100]\n",
    "x2=-b/w[1]+np.dot (-w[0]/w[1],x1)\n",
    "\n",
    "plt.plot(x1, x2, color='black')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Exam1 Score')\n",
    "plt.ylabel('Exam2 Score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea33cd4",
   "metadata": {},
   "source": [
    "# Expected Output\n",
    "\n",
    "<img src=\"A3image2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd689d40",
   "metadata": {},
   "source": [
    "# 5 (1 point)\n",
    "\n",
    "Use the learned logistic regression model to determine if a student with scores (70,50) is likely to get admitted to SDSU and if a student with scores (60,58) is likely to get admitted to SDSU. Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03fa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start your code\n",
    "\n",
    "X0 = 70\n",
    "X1 = 50\n",
    "\n",
    "if (b + w[0] * X0 + w[1] * X1) > 0:\n",
    "    print('Student with score (70,50) Likely to Get Admitted')\n",
    "else:\n",
    "    print('Student with score (70,50) Not Likely to Get Admitted')\n",
    "    \n",
    "#End your code\n",
    "X0 = 60\n",
    "X1 = 58\n",
    "\n",
    "#Start your code\n",
    "\n",
    "if (b + w[0] * X0 + w[1] * X1) > 0:\n",
    "    print('Student with score (60,58) Likely to Get Admitted')\n",
    "else:\n",
    "    print('Student with score (60,58) Not Likely to Get Admitted')\n",
    "\n",
    "#End your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8188298",
   "metadata": {},
   "source": [
    "# Expected Result\n",
    "\n",
    "Student with score (70,50) Likely to Get Admitted <br>\n",
    "Student with score (60,58) Not Likely to Get Admitted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa360ff2",
   "metadata": {},
   "source": [
    "## Determine Classification Predictions Based on Learned Logistic Regression Model\n",
    "\n",
    "\n",
    "Using the learned parameters, b, w1 and w2, for each training data sample, predict the class 1 or 0 and store in array y_pred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60619583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty array y_pred to store the predicted classifications\n",
    "y_pred = np.zeros(100)\n",
    "\n",
    "# Iterate through each row of X to calculate predictions\n",
    "for i in range(100):\n",
    "  # Calculate the linear combination of features and bias\n",
    "  z = w[0] * X_train[i, 0] + w[1] * X_train[i, 1] + b\n",
    "\n",
    "  # Apply the sigmoid function to get the predicted probability\n",
    "  probability = 1 / (1 + np.exp(-z))\n",
    "\n",
    "  # Classify as 1 if probability is above 0.5, otherwise as 0\n",
    "  y_pred[i] = 1 if probability > 0.5 else 0\n",
    "\n",
    "# Now, the array y_pred contains the predicted classifications for each sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb64f4",
   "metadata": {},
   "source": [
    "# 6. Calculate evaluation metrics\n",
    "\n",
    "**2 points**\n",
    "\n",
    "From C and actual $y$ values for each of the 100 samples, calculate the 8 evaluation metrics.\n",
    "\n",
    "**NOTE**: We assume that label y = 1 is positive, and y = 0 is negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09315d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7.\n",
    "# Calculate TP, FP, TN, FN, Accuracy, Precision, Recall, and F-1 score\n",
    "# We assume that label y = 1 is positive, and y = 0 is negative\n",
    "def calc_metrics(Y_actual, Y_pred):\n",
    "    \"\"\"\n",
    "    Calculate metrics\n",
    "    \n",
    "    Args:\n",
    "    Y_actual -- test label\n",
    "    Y_pred_sigmoid -- predictions on test data\n",
    "    \n",
    "    Return:\n",
    "    metrics -- a dict object\n",
    "    \"\"\"\n",
    "    #Calculate Y_pred = 1 or 0 depending on Y_pred_Sigmoid >0 or <=0 respectively\n",
    "    \n",
    "    assert(Y_actual.shape == Y_pred.shape)\n",
    "    \n",
    "    ##### START YOUR CODE #####\n",
    "    \n",
    "    \n",
    "    ##### END YOUR CODE #####\n",
    "    \n",
    "    metrics = {\n",
    "        'TP': TP,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'FN': FN,\n",
    "        'Accuracy': Accuracy,\n",
    "        'Precision': Precision,\n",
    "        'Recall': Recall,\n",
    "        'F1': F1\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Evaluate Task 7\n",
    "m = calc_metrics(y_train, y_pred)\n",
    "print('TP = {}, FP = {}, TN = {}, FN = {}, \\nAccuracy = {}, Precision = {}, Recall = {}, F1 = {}'.format(\n",
    "    m['TP'], m['FP'], m['TN'], m['FN'], m['Accuracy'], m['Precision'], m['Recall'], m['F1']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687972ab",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "TP = 58, FP = 6, TN = 34, FN = 2,<br> \n",
    "Accuracy = 0.92, Precision = 0.90625, Recall = 0.9666666666666667, F1 = 0.9354838709677419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb826c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
