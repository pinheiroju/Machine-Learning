{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS549 Machine Learning - Spring 2024- Irfan Khan \n",
    "# Assignment 5: *K*-means clustering algorithm on image data\n",
    "\n",
    "Updated from assignment designed by Yang Xu, Ex-Assistant Professor of Computer Science, San Diego State University\n",
    "\n",
    "**Total points: 10**\n",
    "\n",
    "In this assignment, you will practice implementing K-means clustering, and then apply it on a subset of sign language dataset. In particular, we use the first three signs contained in the first 491 rows of X_train.npy.\n",
    "\n",
    "The `PCA` module provided by `sklearn` package will be used for pre-clustering analysis and post-clustering visualization. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load data and preprocess - basically create a 2D array with 491 rows corresponding to the images for first three signs. The columns would contain the image representation. 64 x 64 pixels per image = 4096 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.load(open('X_train.npy', 'rb'))\n",
    "\n",
    "\n",
    "X = X_all[:491,:]\n",
    "\n",
    "print(X_all.shape)\n",
    "\n",
    "X = X.reshape(491, -1)\n",
    "\n",
    "print('Shape of X:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected output \n",
    "\n",
    "(1644, 64, 64, 1)<br>\n",
    "Shape of X: (491, 4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis\n",
    "\n",
    "First, we reduce and dimension of the original data to 2, and plot it. The goal of this step is to have some clue of what $k$ values to use, i.e., the number of clusters.\n",
    "\n",
    "We know that image data of 3 classes are selected, but we use them as if they are unlabelled. Judging from the 2-D plot, there are quite amount of outliers in data, and choosing $k=3$ may well group those outliers into a cluster, instead of grouping into the correct classes. For this we will find out at the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_proj = pca.transform(X)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()\n",
    "plt.scatter(X_proj[:,0], X_proj[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Output\n",
    "\n",
    "<img src=\"A6image2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1. Initialize centroids\n",
    "**1 points**\n",
    "\n",
    "The first step of k-means is to randomly initialize a set of centroids. To accomplish this, we simply select $k$ out of the $m$ data points randomly.\n",
    "\n",
    "**Instructions:**\n",
    "- Data are stored in rows in `X`. We draw `k` random rows out of it by calling `numpy.random.choice()`. Notice that use the argument `replace=False` is important. Otherwise, it is possible to sample repeated points.\n",
    "- The returned `centroids` are in shape (k, n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize centroids\n",
    "def init_centroids(X, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- data, shape (m,n)\n",
    "    k -- number of clusters\n",
    "    \n",
    "    Return:\n",
    "    centroids -- k randomly picked data points as initial centroids, shape (k,n)\n",
    "    \"\"\"\n",
    "    assert(k > 1)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 1\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "X_tmp = np.random.randn(10,4)\n",
    "c = init_centroids(X_tmp, k=3)\n",
    "\n",
    "print ('X_tmp', X_tmp)\n",
    "\n",
    "print('Shape of centroids:', c.shape)\n",
    "print('centroids:', c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected output\n",
    "\n",
    "X_tmp [[ 1.62434536 -0.61175641 -0.52817175 -1.07296862]<br>\n",
    " [ 0.86540763 -2.3015387   1.74481176 -0.7612069 ]<br>\n",
    " [ 0.3190391  -0.24937038  1.46210794 -2.06014071]<br>\n",
    " [-0.3224172  -0.38405435  1.13376944 -1.09989127]<br>\n",
    " [-0.17242821 -0.87785842  0.04221375  0.58281521]<br>\n",
    " [-1.10061918  1.14472371  0.90159072  0.50249434]<br>\n",
    " [ 0.90085595 -0.68372786 -0.12289023 -0.93576943]<br>\n",
    " [-0.26788808  0.53035547 -0.69166075 -0.39675353]<br>\n",
    " [-0.6871727  -0.84520564 -0.67124613 -0.0126646 ]<br>\n",
    " [-1.11731035  0.2344157   1.65980218  0.74204416]]<br>\n",
    "Shape of centroids: (3, 4)<br>\n",
    "centroids: [[ 0.3190391  -0.24937038  1.46210794 -2.06014071]<br>\n",
    " [-1.11731035  0.2344157   1.65980218  0.74204416]<br>\n",
    " [ 0.90085595 -0.68372786 -0.12289023 -0.93576943]]<br>\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## Task 2. Compute distance between data points and centroids\n",
    "**3 points**\n",
    "\n",
    "Next, we need to compute the distances between data points and centroids. More concretely, for each data point `X[i,:]`, we need to compute its distance from the $k$ centroids, i.e., `centroids[j,:]` ($j=1,2,\\dots,k$). We will store the computed distances in a $k\\times m$ array, in which the element at position $(i,j)$ is the distance between `centroids[i,:]` and `X[j,:]`. We use Euclidean distance.\n",
    "\n",
    "There are multiple ways of implementing this computation. For example:\n",
    "\n",
    "You create an empty array distances of shape (k, m).\n",
    "Then you use a for loop, for j in range(k):, and in each step, you compute S = X - centroids[j,:] followed by S**2, numpy.sum(), and numpy.sqrt() to get the Euclidean distance, which is stored in a (1,m) array d. Then you copy d back to the jth row of distances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- data, shape (m,n)\n",
    "    centroids -- shape (k,n)\n",
    "    \n",
    "    Return:\n",
    "    distances -- shape (k,m)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 2\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.randn(5,4)\n",
    "c = init_centroids(X_tmp, k=2)\n",
    "\n",
    "\n",
    "dists = compute_distances(X_tmp, c)\n",
    "print('Distances:', dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "Distances: [[2.60208956 2.50540693 0.         1.208094   3.10448915]<br>\n",
    " [2.94872855 0.         2.50540693 2.36130342 2.79449534]]<br>\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## Task 3. Find the closest centroid for each data point\n",
    "**2 point**\n",
    "\n",
    "Given the distances computed, we can find the closest centroid for each data point. We store this information in a $m\\times 1$ array, and each element is the index of the closest centroid, i.e., an integer ranging from $0$ to $k-1$.\n",
    "\n",
    "**Instructions:**\n",
    "- You can apply `numpy.argmin()` on the `distances` computed in previous step as input, and a proper `axis` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest centroid for each data point\n",
    "def closest_centroid(distances):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    distances -- numpy array of shape (k,m), output of compute_distances()\n",
    "    \n",
    "    Return:\n",
    "    indices -- numpy array of shape (1,m)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 3\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.randn(5,4)\n",
    "c = init_centroids(X_tmp, k=2)\n",
    "\n",
    "dists = compute_distances(X_tmp, c)\n",
    "closest_indices = closest_centroid(dists)\n",
    "\n",
    "print('Indices of the closest centroids:', closest_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "Indices of the closest centroids: [0 1 0 0 1]<br>\n",
    "\n",
    "\n",
    "\n",
    "## Task 4. Update centroids\n",
    "**2 points**\n",
    "\n",
    "Given the indices of closest centroid for each data point, you need to update the centroids by computing the average positions of the data points belonging to each cluster ($1,2,\\dots,k$).\n",
    "\n",
    "**Instructions:**\n",
    "- Because `closest_indices` (output of `closes_centroid()`) is of shape (1,m), you can access the data points whose closest centroid is `j` by using the slice `X[closest_indices==j,:]`.\n",
    "- Pay attention to the dimension of `new_centroids` computed, and it needs to be the same as `centroids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update centroids\n",
    "def update_centroids(X, closest_indices, centroids):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- data, shape (m,n)\n",
    "    closest_indices -- output of closest_centroid()\n",
    "    centroids -- old centroids positions\n",
    "    \n",
    "    Return:\n",
    "    new_centroids -- new centroids positions, shape (k,n)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### \n",
    "    \n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    assert(centroids.shape == new_centroids.shape)\n",
    "    \n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 4\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.randn(4,5)\n",
    "c = init_centroids(X_tmp, k=2)\n",
    "\n",
    "dists = compute_distances(X_tmp, c)\n",
    "closest_indices = closest_centroid(dists)\n",
    "new_c = update_centroids(X_tmp, closest_indices, c)\n",
    "\n",
    "print('New centroids:', new_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "New centroids: [[-1.70071498  0.78619178 -0.81953266  0.18062642  0.16672242]<br>\n",
    " [ 1.54322665 -1.33594856 -0.42529448 -0.72851149  0.99958854]]\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## Task 5. Integrated model\n",
    "**2 points**\n",
    "\n",
    "Finally, we combine all the previous steps into one model. We repeatedly find the closest centroid for each data points, and then update the centroids, until the centroids no longer change. The final stable `closest_indices` is then the clustering result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "def kmeans(X, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- data, shape (m,n)\n",
    "    k -- number of clusters\n",
    "    \n",
    "    Return:\n",
    "    closest_indices -- final assignment of clusters to each data point, shape (1, m)\n",
    "    centroids -- final positions of centroids\n",
    "    \"\"\"\n",
    "    centroids = init_centroids(X, k)\n",
    "    \n",
    "    old_centroids = None\n",
    "    while not np.array_equal(old_centroids, centroids):\n",
    "        # Backup centroids\n",
    "        old_centroids = np.copy(centroids)\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # Compute distances\n",
    "        \n",
    "        \n",
    "        # Find closest centroid\n",
    "        \n",
    "        \n",
    "        # Update centroids\n",
    "        \n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "    return closest_indices, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 5\n",
    "closest_indices, centroids = kmeans(X, 3)\n",
    "\n",
    "print('closest_indices[:10]', closest_indices[:10])\n",
    "print('closest_indices[70:80]', closest_indices[70:80])\n",
    "print('closest_indices[140:150]', closest_indices[140:150])\n",
    "print('closest_indices[210:220]', closest_indices[210:220])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "closest_indices[:10] [1 1 0 0 1 1 1 1 1 0]<br>\n",
    "closest_indices[70:80] [1 1 1 1 1 1 1 1 1 1]<br>\n",
    "closest_indices[140:150] [1 1 1 1 1 1 1 1 1 0]<br>\n",
    "closest_indices[210:220] [2 1 1 1 2 2 2 2 2 2]\n",
    "\n",
    "***\n",
    "\n",
    "## Visualize clustering result using PCA from sklearn library (ungraded) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#For visualization, project to 2 PCs\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "X_proj = pca.transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 3.5))\n",
    "\n",
    "closest_indices, centroids = kmeans(X, 3)\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.scatter(X_proj[closest_indices==0, 0], X_proj[closest_indices==0, 1])\n",
    "plt.scatter(X_proj[closest_indices==1, 0], X_proj[closest_indices==1, 1])\n",
    "plt.scatter(X_proj[closest_indices==2, 0], X_proj[closest_indices==2, 1])\n",
    "plt.title('Clustering result of k=3')\n",
    "\n",
    "closest_indices, centroids = kmeans(X, 4)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.scatter(X_proj[closest_indices==0, 0], X_proj[closest_indices==0, 1])\n",
    "plt.scatter(X_proj[closest_indices==1, 0], X_proj[closest_indices==1, 1])\n",
    "plt.scatter(X_proj[closest_indices==2, 0], X_proj[closest_indices==2, 1])\n",
    "plt.scatter(X_proj[closest_indices==3, 0], X_proj[closest_indices==3, 1])\n",
    "plt.title('Clustering result of k=4')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Result\n",
    "\n",
    "<img src = \"A6image3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    " Let's compare with the ground truth, i.e., labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3.5))\n",
    "\n",
    "plt.scatter(X_proj[:163, 0], X_proj[:163, 1], c='blue')\n",
    "plt.scatter(X_proj[163:327, 0], X_proj[163:327, 1], c='green')\n",
    "plt.scatter(X_proj[327:491, 0], X_proj[327:491, 1], c='orange')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Result\n",
    "\n",
    "<img src = \"A6image4.png\">\n",
    "\n",
    "Roughly speaking, looks like k-means (k=3)distinguishes well between class 1 and class 2 as long as they are in the big cluster. Otherwise it doesn't do too well and doesn't deal well with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly using the KMeans function from sklearn (Ungraded)\n",
    "\n",
    "Double check the results obtained earlier with the results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X is your data matrix of shape (m samples, n features)\n",
    "# Specify the number of clusters (k)\n",
    "k = 3\n",
    "\n",
    "np.random.seed(1)\n",
    "initial_centroids = X[np.random.choice(X.shape[0], k, replace=False),:]\n",
    "    \n",
    "# Create a KMeans instance\n",
    "kmeans = KMeans(n_clusters=k, init=initial_centroids, n_init=1)\n",
    "\n",
    "# Fit the KMeans model to your data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Obtain cluster labels for each data point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Obtain cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "print('closest_indices[:10]', labels[:10])\n",
    "print('closest_indices[70:80]', labels[70:80])\n",
    "print('closest_indices[140:150]', labels[140:150])\n",
    "print('closest_indices[210:220]', labels[210:220])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Result\n",
    "\n",
    "<img src = \"A6image5.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
